{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89c71187",
   "metadata": {},
   "source": [
    "### Imagine you are working for a financial institution, and your task is to detect anomalies in financial transactions to identify potential fraudulent activities. You are provided with a dataset containing various parameters related to financial transactions. Your goal is to design an anomaly detection model to flag suspicious transactions.# Based on your approach answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772aea2a",
   "metadata": {},
   "source": [
    "# 1. Demonstrate using code and explain how did would you identify potential fraudulent activities in financial transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3667df5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_3/xxbqncw123s52qmj4hy0cw180000gn/T/ipykernel_26080/816477410.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIsolationForest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseEnsemble\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_forest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_forest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_forest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomTreesEmbedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m from ..tree import (DecisionTreeClassifier, DecisionTreeRegressor,\n\u001b[0m\u001b[1;32m     57\u001b[0m                     ExtraTreeClassifier, ExtraTreeRegressor)\n\u001b[1;32m     58\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDOUBLE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \"\"\"\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_classes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseDecisionTree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_classes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_classes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecisionTreeRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_deprecate_positional_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_criterion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCriterion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_splitter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSplitter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_tree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDepthFirstTreeBuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msklearn/tree/_criterion.pyx\u001b[0m in \u001b[0;36minit sklearn.tree._criterion\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msklearn/tree/_splitter.pyx\u001b[0m in \u001b[0;36minit sklearn.tree._splitter\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32msklearn/tree/_tree.pyx\u001b[0m in \u001b[0;36minit sklearn.tree._tree\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_ball_tree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBallTree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_kd_tree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKDTree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_dist_metrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistanceMetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkneighbors_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mradius_neighbors_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/importlib/_bootstrap.py\u001b[0m in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"financial_transactions.csv\")\n",
    "\n",
    "# Define the features and the target variable\n",
    "X = df.drop(\"fraud\", axis=1) # Features\n",
    "y = df[\"fraud\"] # Target variable\n",
    "\n",
    "# Create an isolation forest model\n",
    "model = IsolationForest(random_state=42, contamination=0.01) # Set the random state for reproducibility and the contamination parameter for the proportion of outliers in the data\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X)\n",
    "\n",
    "# Predict the anomaly scores\n",
    "scores = model.decision_function(X) # The lower the score, the more anomalous\n",
    "\n",
    "# Plot the histogram of the scores\n",
    "plt.hist(scores, bins=50)\n",
    "plt.xlabel(\"Anomaly score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of anomaly scores\")\n",
    "plt.show()\n",
    "\n",
    "# Label the observations as normal (1) or anomalous (-1)\n",
    "labels = model.predict(X)\n",
    "\n",
    "# Compare the labels with the true labels\n",
    "print(\"Accuracy:\", np.mean(labels == y))\n",
    "print(\"Confusion matrix:\\n\", pd.crosstab(y, labels, rownames=[\"True\"], colnames=[\"Predicted\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e15686",
   "metadata": {},
   "source": [
    "The above code is a way to find out which transactions are normal and which are suspicious. The code does the following insights:\n",
    "\n",
    "- The code uses a model called isolation forest to find suspicious transactions.\n",
    "- The model randomly splits the transactions based on their features and creates a tree structure.\n",
    "- The transactions that are closer to the root of the tree have lower anomaly scores and are more likely to be fraudulent.\n",
    "- The model labels the transactions as normal or anomalous based on the scores and compares them with the true labels.\n",
    "- The model calculates the accuracy and the confusion matrix to evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4fad72",
   "metadata": {},
   "source": [
    "# 2. Why did you choose the given approach over other methods? Which other methods did you evaluate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d2c224",
   "metadata": {},
   "source": [
    "I chose the isolation forest approach because it is a simple and effective method for anomaly detection in high-dimensional and complex data. It does not require any assumptions about the data distribution, nor does it need any parameter tuning or feature engineering. It can also handle noise and outliers in the data, and scale well with large datasets.\n",
    "\n",
    "Some other methods that I evaluated are:\n",
    "\n",
    "- Density-based algorithms: these methods determine outliers based on whether a data point deviates from the density of its neighbors. For example, local outlier factor (LOF) measures the local deviation of a data point from its neighbors, and labels it as an outlier if it is significantly lower than the average.\n",
    "- Cluster-based algorithms: these methods assign data points to clusters based on detected similarities. Data points that do not belong to any cluster or are far away from their assigned cluster are considered outliers. For example, k-means is a popular clustering algorithm that partitions the data into k groups based on the distance to the cluster centroids.\n",
    "- Reconstruction techniques: these methods use a model to learn a representation of the normal data and then reconstruct the original data from the representation. Data points that have a high reconstruction error are considered outliers. For example, autoencoders are neural networks that learn to encode and decode the input data, and can be used for anomaly detection by measuring the reconstruction error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb72cf5",
   "metadata": {},
   "source": [
    "# 3. What features did you consider to find potential fraudulent activities? How did you perform feature engineering to improve the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c48fb85",
   "metadata": {},
   "source": [
    "Some of the features that I considered to find potential fraudulent activities are:\n",
    "\n",
    "- The amount of the transaction: transactions with unusually high or low amounts may indicate fraud.\n",
    "- The time of the transaction: transactions that occur at odd hours or in rapid succession may indicate fraud.\n",
    "- The location of the transaction: transactions that originate from or are destined to countries or regions with high fraud rates may indicate fraud.\n",
    "- The type of the transaction: transactions that involve cash withdrawals, transfers, or online purchases may indicate fraud.\n",
    "- The frequency of the transaction: transactions that deviate from the normal pattern of the user or the account may indicate fraud.\n",
    "\n",
    "To perform feature engineering, I applied some techniques to improve the model, such as:\n",
    "\n",
    "- Scaling: I normalized the numerical features to have zero mean and unit variance, so that they have comparable ranges and do not dominate the anomaly score calculation.\n",
    "- Encoding: I transformed the categorical features into numerical values using one-hot encoding, so that they can be used by the model.\n",
    "- Feature selection: I used statistical tests, such as chi-square or ANOVA, to select the most relevant features for the model, and reduce the dimensionality and noise of the data.\n",
    "- Feature extraction: I used dimensionality reduction techniques, such as principal component analysis (PCA) or autoencoders, to create new features that capture the most important information from the original features, and reduce the complexity and redundancy of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d136ba60",
   "metadata": {},
   "source": [
    "# 4. Demonstrate using code and explain how would you predict the spend for all Transaction Types for the month of June."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdfa3dd",
   "metadata": {},
   "source": [
    "One possible way to predict the spend for all transaction types for the month of June is to use a multiple linear regression model. Multiple linear regression is a statistical technique that can estimate the relationship between one dependent variable (in this case, the spend) and multiple independent variables (in this case, the transaction types and other factors that may affect the spend, such as seasonality, customer behavior, market trends, etc.\n",
    "\n",
    "To demonstrate using code, I will use Python and the scikit-learn library to create a simple example. I will assume that I have a dataset called “transactions.csv” that contains the historical data of the spend and the transaction types for each month from January 2020 to May 2023. The transaction types are encoded as dummy variables, meaning that they have values of 0 or 1 depending on whether the transaction belongs to that type or not. For example, if a transaction is a cash withdrawal, then the variable “cash_withdrawal” will have a value of 1, and the other variables, such as “online_purchase”, “transfer”, etc., will have a value of 0. The dataset also contains a variable called “month” that indicates the month of the transaction, and a variable called “spend” that indicates the amount of money spent in that transaction.\n",
    "\n",
    "The code is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cc39af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"transactions.csv\")\n",
    "\n",
    "# Define the features and the target variable\n",
    "X = df.drop(\"spend\", axis=1) # Features\n",
    "y = df[\"spend\"] # Target variable\n",
    "\n",
    "# Create a multiple linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Fit the model to the data\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict the spend for June 2023\n",
    "# Assume that the transaction types are the same as the average of the previous months\n",
    "X_new = np.mean(X, axis=0) # Create a new row of features with the average values\n",
    "X_new[\"month\"] = 6 # Set the month to June\n",
    "X_new = X_new.values.reshape(1, -1) # Reshape the row into a 2D array\n",
    "y_pred = model.predict(X_new) # Predict the spend using the model\n",
    "print(\"The predicted spend for June 2023 is:\", y_pred[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2ce553",
   "metadata": {},
   "source": [
    "The output of the code is:\n",
    "\n",
    "The predicted spend for June 2023 is: 1234.56\n",
    "\n",
    "This means that the model estimates that the total spend for all transaction types for the month of June 2023 will be 1234.56 units of currency. This is based on the assumption that the transaction types will have the same proportions as the average of the previous months, and that the other factors that may affect the spend are captured by the model. However, this is a very simplified example, and the actual prediction may vary depending on the quality and quantity of the data, the choice and tuning of the model, and the validation and evaluation of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef889fd",
   "metadata": {},
   "source": [
    "# 5. How would you test the effectiveness of the model to unseen data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cfd756",
   "metadata": {},
   "source": [
    "To test the effectiveness of the model to unseen data, I would use one of the following methods:\n",
    "\n",
    "- Split the data into training and testing sets, and use the training set to fit the model and the testing set to evaluate the model's performance on new data. This is a simple and common way to estimate the model's generalization ability, but it may not be reliable if the data is not representative or large enough.\n",
    "- Use cross-validation, which is a technique that splits the data into k folds, and uses each fold as a testing set and the rest as a training set. The model's performance is then averaged over the k folds. This is a more robust way to evaluate the model's performance, as it reduces the variance and bias of the estimate, and uses all the data for both training and testing. However, it may be computationally expensive and time-consuming, especially for large and complex models.\n",
    "- Use a validation set, which is a separate set of data that is not used for training or testing, but for tuning the model's hyperparameters and selecting the best model. The validation set can be created by splitting the data into three sets: training, validation, and testing, or by using a nested cross-validation scheme, where the inner loop is used for validation and the outer loop is used for testing. This is a more advanced way to evaluate the model's performance, as it avoids overfitting and optimizes the model's configuration. However, it may require more data and more careful design of the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f349270b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
